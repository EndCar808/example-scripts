{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, Numerai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome to the Numerai Data Science Tournament!\n",
    "\n",
    "This notebook is designed to help you build your first machine learning model and start competing the tournament. \n",
    "\n",
    "In this notebook we will\n",
    "1. Download and explore the Numerai dataset\n",
    "2. Train and evaluate your first machine learning model\n",
    "3. Deploy your model to start making live submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle\n",
    "\n",
    "# Inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset  \n",
    "\n",
    "At a high level, the Numerai dataset is a tabular dataset that describes the stock market over time. \n",
    "\n",
    "Each row represents a stock at a specific point in time, where `id` is the stock id and the `era` is the date. The `features` describe the attributes of the stock (eg. P/E ratio) known on the date and the `target` is a measure of 20-day returns.\n",
    "\n",
    "The unique thing about Numerai's dataset is that it is `obfuscated`, which means that the underlying stock ids, feature names, and target definitions are anonymized. This makes it so that we can give this data out for free and so that it can be modeled without any financial domain knowledge (or bias!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset\n",
    "Let's download the historical training data and take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NumerAPI - the official Python API client for Numerai\n",
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "\n",
    "# Print all files available for download in the latest dataset\n",
    "[f for f in napi.list_datasets() if f.startswith(\"v4.2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Download the training data and feature metadata\n",
    "# This will take a few minutes üçµ\n",
    "napi.download_dataset(\"v4.2/train_int8.parquet\");\n",
    "napi.download_dataset(\"v4.2/features.json\");\n",
    "\n",
    "# Load only the \"medium\" feature set to reduce memory usage and speedup model training (required for Colab free tier)\n",
    "# Use the \"all\" feature set to use all features \n",
    "feature_metadata = json.load(open(\"v4.2/features.json\"))\n",
    "feature_cols = feature_metadata[\"feature_sets\"][\"medium\"]\n",
    "train = pd.read_parquet(\"v4.2/train_int8.parquet\", columns=[\"era\"] + feature_cols + [\"target\"])\n",
    "\n",
    "# Downsample to every 4th era to reduce memory usage and speedup model training (suggested for Colab free tier)\n",
    "# Comment out the line below to use all the data \n",
    "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]\n",
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eras\n",
    "As mentioned above, each `era` corresponds to a different date. Each era is exactly 1 week apart.\n",
    "\n",
    "It is helpful to think about rows of stocks within the same `era` as a single example. You will notice that throughout this notebook and other examples, we often talk about things \"per era\". For example, the number of rows per era represents the number of stocks in Numerai's investable universe on that date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of rows per era\n",
    "train.groupby(\"era\").size().plot(title=\"Number of rows per era\", figsize=(5, 3), xlabel=\"Era\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "As mentioned above, `features` are quantitative attributes of each stock: fundamentals like P/E ratio, technical signals like RSI, market data like short interest, secondary data like analyst ratings, and much more. \n",
    "\n",
    "The underlying definition of each feature is not important, just know that Numerai has included these features in the dataset because we believe they are predictive of the `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features \n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature values are binned into 5 equal bins: `0`, `1`, `2`, `3`, `4`. This heavy regularization of feature values is to avoid overfitting as the underlying values are extremely noisy.\n",
    "\n",
    "If data for a particular feature is missing for that era (more common in early `eras`), then all values will be set to `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "first_era = train[train[\"era\"] == train[\"era\"].unique()[0]]\n",
    "last_era = train[train[\"era\"] == train[\"era\"].unique()[-1]]\n",
    "last_era[feature_cols[-1]].plot(kind=\"hist\", title=\"5 equal bins\", density=True, bins=50, ax=ax1);\n",
    "first_era[feature_cols[-1]].plot(kind=\"hist\", title=\"missing data\", density=True, bins=50, ax=ax2);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target\n",
    "The `target` is a measure of 20-day stock market returns. Specifically, it is a measure of \"stock-specific\" returns that are not \"explained\" by broader trends in the market, country, sector, or well-known \"factors\".\n",
    "\n",
    "Target values are binned into 5 unequal bins: `0`, `0.25`, `0.5`, `0.75`, `1.0`. Again, this heavy regularization of target values is to avoid overfitting as the underlying values are extremely noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot density histogram of the target\n",
    "train[\"target\"].plot(kind=\"hist\", title=\"Target\", figsize=(5, 3), xlabel=\"Value\", density=True, bins=50);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "At a high level, our task is to model and predict the `target` variable.\n",
    "\n",
    "### Model training\n",
    "\n",
    "You are free to use any tool or framework, but here we will be using LGBMRegressor, a popular choice amongst tournament participants.\n",
    "\n",
    "While you wait for the model to train, watch this [video](https://www.youtube.com/watch?v=w8Y7hY05z7k) to learn why tree-based models work so well on tabular datasets from our Chief Scientist MDO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html \n",
    "import lightgbm as lgb\n",
    "\n",
    "# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "model = lgb.LGBMRegressor(\n",
    "  n_estimators=2000,\n",
    "  learning_rate=0.01,\n",
    "  max_depth=5,\n",
    "  num_leaves=2**5-1,\n",
    "  colsample_bytree=0.1\n",
    ")\n",
    "\n",
    "# This will take a few minutes üçµ\n",
    "model.fit(\n",
    "  train[feature_cols],\n",
    "  train[\"target\"]\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation predictions\n",
    "\n",
    "Now let's make some out-of-sample predictions on the validation dataset to evaluate our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download validation data \n",
    "# This will take a few minutes üçµ\n",
    "napi.download_dataset(\"v4.2/validation_int8.parquet\");\n",
    "\n",
    "# Load the validation data, filtering for data_type == \"validation\"\n",
    "validation = pd.read_parquet(\"v4.2/validation_int8.parquet\", columns=[\"era\", \"data_type\"] + feature_cols + [\"target\"]) \n",
    "validation = validation[validation[\"data_type\"] == \"validation\"]\n",
    "del validation[\"data_type\"]\n",
    "\n",
    "# Downsample to every 4th era to reduce memory usage and speedup evaluation (suggested for Colab free tier)\n",
    "# Comment out the line below to use all the data (higher memory usage, slower inference, more accurate evaluation)\n",
    "validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\n",
    "\n",
    "# Eras are 1 week apart, but targets look 4 weeks into the future, so we need to \"embargo\" the 4 eras following our last train era to avoid data leakage. \n",
    "last_train_era = int(train[\"era\"].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
    "\n",
    "# Generate predictions against the out-of-sample validation features\n",
    "# This will take a few minutes üçµ\n",
    "validation[\"prediction\"] = model.predict(validation[feature_cols])\n",
    "validation[[\"era\", \"prediction\", \"target\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation\n",
    "The primary scoring metric in Numerai is called `numerai_corr` or `CORR`, which is a Numerai specific variant of the Pearson Correlation Coefficient. \n",
    "\n",
    "This metric is designed to \"align incentives\" between model and hedge fund performance. A model with a good `CORR` score should help the hedge fund make good returns.\n",
    "\n",
    "On the Numerai website you will also see this score referred to as `CORR20V2`, where the \"20\" refers to the 20-day return target and \"v2\" specifies that we are using the 2nd version of the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Numerai's primary scoring metric\n",
    "def numerai_corr(preds, target):\n",
    "    # rank (keeping ties) then gaussianize predictions to standardize prediction distributions\n",
    "    ranked_preds = (preds.rank(method=\"average\").values - 0.5) / preds.count()\n",
    "    gauss_ranked_preds = stats.norm.ppf(ranked_preds)\n",
    "    # center targets around 0\n",
    "    centered_target = target - target.mean()\n",
    "    # raise both preds and target to the power of 1.5 to accentuate the tails\n",
    "    preds_p15 = np.sign(gauss_ranked_preds) * np.abs(gauss_ranked_preds) ** 1.5\n",
    "    target_p15 = np.sign(centered_target) * np.abs(centered_target) ** 1.5\n",
    "    # finally return the Pearson correlation\n",
    "    return np.corrcoef(preds_p15, target_p15)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, it is important for us to score each historical `era` independantly. So when evaluating the performance of our model, we should be looking at the \"per era\" `corr`.\n",
    "\n",
    "One thing you may notice here is how low the scores are (in the range of +/- 5% correlation). This is very normal in the domain of quantitative finance and is part of the reason why we say Numerai is the \"hardest data science tournament\" in the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the per-era correlation between our predictions and the target values\n",
    "per_era_corr = validation.groupby(\"era\").apply(lambda x: numerai_corr(x[\"prediction\"], x[\"target\"]))\n",
    "\n",
    "# Plot the per-era correlation\n",
    "per_era_corr.plot(kind=\"bar\", title=\"Validation Correlation\", figsize=(10, 6), xticks=[], snap=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at the `corr` score for each era, it is helpful to look at the cumulative `corr`. \n",
    "\n",
    "If you are familiar with \"backtesting\" in quant finance where people simulate the historical performance of their investment strategies, you can roughly think of this plot as a backtest of your model performance over the historical validation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative per-era correlation\n",
    "per_era_corr.cumsum().plot(kind=\"line\", title=\"Cumulative Validation Correlation\", figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics \n",
    "\n",
    "To evaluate the performance of our model, it is also helpful to compute some summary metrics over the entire validation period.\n",
    "\n",
    "`Mean` of correlations is the primary measure of your model's performance.\n",
    "\n",
    "`Sharpe` is a measure of your model's consistency, a concept borrowed from finance where it usually refers to risk adjusted returns of an investment strategy. In Numerai, we compute sharpe as the average correlation divided by the standard deviation of correlations.\n",
    "\n",
    "`Max drawdown` is a measure of your model's risk, another concept borrowed from finance where it usually refers to the maximum financial loss suffered by an investment strategy. In Numerai, we compute max drawdown as the maximum peak to trough drop in cumulative validation correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance metrics\n",
    "corr_mean = per_era_corr.mean()\n",
    "corr_std = per_era_corr.std(ddof=0)\n",
    "corr_sharpe = corr_mean / corr_std\n",
    "max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"mean\": corr_mean,\n",
    "    \"std\": corr_std,\n",
    "    \"sharpe\": corr_sharpe,\n",
    "    \"max_drawdown\": max_drawdown\n",
    "}, index=[\"Value\"]).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These performance metrics above is not amazing but good enough for us to get started. Don't worry, we will be learning how to improve our model performance in the next tutorials!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Submissions \n",
    "\n",
    "Unlike Kaggle competitions that evalute models based on <ins>test</ins> performance, Numerai evaluates models based based on <ins>live</ins> performance. \n",
    " \n",
    "### Live predictions\n",
    "\n",
    "Every Tuesday-Saturday, new `live features` are released, which represent the current state of the stock market. \n",
    "\n",
    "Your task is to generate `live predictions` on the unknown target values, which represent stock market returns 20 days into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest live features \n",
    "napi.download_dataset(\"v4.2/live_int8.parquet\")\n",
    "\n",
    "# Load live features\n",
    "live_features = pd.read_parquet(f\"v4.2/live_int8.parquet\", columns=feature_cols)\n",
    "\n",
    "# Generate live predictions\n",
    "live_predictions = model.predict(live_features[feature_cols])\n",
    "\n",
    "# Format submission\n",
    "pd.Series(live_predictions, index=live_features.index).to_frame(\"prediction\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model upload\n",
    "\n",
    "To participate in the tournament, you must submit live predictions every Tuesday-Saturday. \n",
    "\n",
    "To automate this process, you can simply:\n",
    "- Define your prediction pipeline as a function\n",
    "- Serialize your function using the `cloudpickle` library\n",
    "- Upload your model pickle file to Numerai\n",
    "- Let Numerai run your model to submit live predictions every day\n",
    "\n",
    "Read more about Model Uploads and other self-hosted automation options in our [docs](https://docs.numer.ai/numerai-tournament/submissions#automation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your prediction pipeline as a function\n",
    "def predict(live_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    live_predictions = model.predict(live_features[feature_cols])\n",
    "    submission = pd.Series(live_predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cloudpickle library to serialize your function\n",
    "import cloudpickle\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"predict.pkl\", \"wb\") as f:\n",
    "    f.write(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('predict.pkl')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You now have a pickle file that is ready for upload.\n",
    "\n",
    "Head back to the [Hello Numerai Tutorial](https://numer.ai/tutorial/hello-numerai) to upload your model! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
